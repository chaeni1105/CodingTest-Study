{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4-1) train_data.csv 파일의 학습데이터 100 개를 이용하여 decision tree (결정 트리)를 학습하시오"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: {0: {3: {0: {1: {0: class    Cat\n",
      "Name: 97, dtype: object, 1: class    Dog\n",
      "Name: 48, dtype: object}}, 1: class    Cat\n",
      "Name: 50, dtype: object}}, 1: {2: {0: class    Dog\n",
      "Name: 0, dtype: object, 1: {1: {0: class    Cat\n",
      "Name: 86, dtype: object, 1: class    Dog\n",
      "Name: 45, dtype: object}}}}}}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "pd_data = pd.read_csv('./train_data.csv') # train_data.csv 파일 읽어 오기\n",
    "\n",
    "features = pd_data.iloc[:100, :-1] # 100개의 데이터만 선택,  features 변수에 할당\n",
    "target = pd_data.iloc[:100, -1:] # # 100개의 레이블만 선택, target 변수에 할당\n",
    "\n",
    "def entropy(y): # 엔트로피 계산 함수 정의\n",
    "    # y에 있는 클래스 레이블들의 엔트로피를 계산합니다.\n",
    "    cnt = np.unique(y, return_counts=True)[1]\n",
    "    probs = cnt / len(y)\n",
    "    entropy = -np.sum(probs * np.log2(probs))\n",
    "    return entropy\n",
    "\n",
    "def information_gain(X, y, feature_idx): # 정보 이득 계산 함수 정의\n",
    "    # feature_idx에 해당하는 특징을 사용하여 분할했을 때의 정보 이득을 계산합니다.\n",
    "    p_entropy = entropy(y)\n",
    "    values, cnt = np.unique(X.iloc[:, feature_idx], return_counts=True)\n",
    "    c_entropy = 0\n",
    "    for value, cnt in zip(values, cnt):\n",
    "        child_y = y[X.iloc[:, feature_idx] == value]\n",
    "        c_entropy += cnt / len(X) * entropy(child_y)\n",
    "    ig = p_entropy - c_entropy\n",
    "    return ig\n",
    "\n",
    "def find_best_feature(X, y): # 가장 정보 이득이 큰 특징을 찾는 함수 정의\n",
    "    # 모든 특징들 중에서 가장 정보 이득이 큰 특징을 찾아 반환합니다.\n",
    "    best_feature = None\n",
    "    best_ig = 0\n",
    "    for i in range(X.shape[1]):\n",
    "        ig = information_gain(X, y, i)\n",
    "        if ig > best_ig:\n",
    "            best_feature = i\n",
    "            best_ig = ig\n",
    "    return best_feature\n",
    "\n",
    "def build_tree(X, y): # Decision tree를 구성하는 함수 정의\n",
    "    # 재귀적으로 decision tree를 구성합니다.\n",
    "    # Base case: 모든 레이블이 동일한 경우\n",
    "    if len(np.unique(y)) == 1:\n",
    "        return y.iloc[0]\n",
    "    # 먼저 base case를 처리하고, 더 이상 분할할 특징이 없을 때, 데이터의 가장 많은 레이블을 반환\n",
    "    if X.shape[1] == 0:\n",
    "        return np.bincount(y).argmax()\n",
    "    # recursive case에서는 find_best_feature 함수를 사용하여 가장 정보 이득이 큰 특징을 찾고, 그 특징의 값을 기준으로 데이터를 분할합니다.\n",
    "    best_feature = find_best_feature(X, y)\n",
    "    tree = {best_feature: {}}\n",
    "    values = np.unique(X.iloc[:, best_feature])\n",
    "    for val in values:\n",
    "        index = X.iloc[:, best_feature] == val\n",
    "        subtree = build_tree(X.loc[index], y.loc[index])\n",
    "        tree[best_feature][val] = subtree\n",
    "    return tree\n",
    "\n",
    "tree = build_tree(features, target) # 트리 만들기\n",
    "print(tree) # 학습된 Decision tree를 출력"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4-2) test_data.csv 파일의 테스트 데이터 10 개에 대한 테스트 결과를 출력하시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test #0 [1, 1, 0, 0] -> class    Dog\n",
      "Name: 0, dtype: object\n",
      "Test #1 [0, 0, 1, 1] -> class    Cat\n",
      "Name: 50, dtype: object\n",
      "Test #2 [1, 1, 0, 0] -> class    Dog\n",
      "Name: 0, dtype: object\n",
      "Test #3 [0, 0, 1, 1] -> class    Cat\n",
      "Name: 50, dtype: object\n",
      "Test #4 [1, 1, 0, 1] -> class    Dog\n",
      "Name: 0, dtype: object\n",
      "Test #5 [1, 1, 1, 0] -> class    Dog\n",
      "Name: 45, dtype: object\n",
      "Test #6 [1, 0, 1, 1] -> class    Cat\n",
      "Name: 86, dtype: object\n",
      "Test #7 [1, 0, 0, 1] -> class    Dog\n",
      "Name: 0, dtype: object\n",
      "Test #8 [0, 1, 1, 1] -> class    Cat\n",
      "Name: 50, dtype: object\n",
      "Test #9 [1, 1, 1, 0] -> class    Dog\n",
      "Name: 45, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd # pandas 불러오기\n",
    "\n",
    "pd_data = pd.read_csv('./test_data.csv') # test_data.csv 파일 읽어오기\n",
    "\n",
    "for i in range(10): # 10번 반복 실행\n",
    "    row = pd_data.iloc[i, :] # i번째 데이터 선택\n",
    "    x = row[:-1] # feature 값\n",
    "    true_label = row[-1] # true label 값\n",
    "    node = tree # decision tree에서 루트 노드를 시작으로 시작\n",
    "    while isinstance(node, dict): # 현재 노드가 dictionary(내부 노드)인 경우, 다음 노드로 이동\n",
    "        feature = list(node.keys())[0] # 현재 노드의 feature 값\n",
    "        value = x[feature] # feature 값에 해당하는 데이터 \n",
    "        node = node[feature][value] # 다음 노드로 이동\n",
    "    pred_label = node # leaf node에 도달하면, 해당 노드에서 반환하는 값을 예측값으로 저장\n",
    "    print(f\"Test #{i} {list(x)} -> {pred_label}\") # 출력"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
